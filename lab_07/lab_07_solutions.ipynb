{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Working with the Transformers Library for Different Natural Language Processing Tasks\n"
      ],
      "metadata": {
        "id": "fOOMgtAuDi4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within this notebook we will use the the **Transformers Library** by [Hugging Face](https://huggingface.co/). The library consists of thousands of pre-trained models many of which are trained on huge datasets for thousands of GPU hours. You can use them either directly for inference (as we will do in this lab session) or fine-tune them for your specific applications. Using pre-trained models allows you to reduce your compute costs and carbon footprint and save time and resources required to develop a model from scratch.\n",
        "\n",
        "For creating user interfaces we will use Gradio which you already know from the previous session on using ML APIs."
      ],
      "metadata": {
        "id": "NqLMXQQ03WIM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NlOTG_js8m0"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The HuggingFace [ModelHub](https://huggingface.co/models) consists of various pre-trained models for different tasks which can be downloaded and used easily using the Transformers Library. "
      ],
      "metadata": {
        "id": "tOTJNeSN5_fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Transformers for Translation Tasks"
      ],
      "metadata": {
        "id": "TL2GbiyXLbYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest way to use a pre-trained model for inference is the **pipeline**. The pipeline can be used out-of-the box for many tasks across modalities (e.g., text, images, etc.). In this lab session we will look into a subset including translation and text classification.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Task</th>\n",
        "    <th>Description</th>\n",
        "    <th>Pipeline identifier</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Translation</td>\n",
        "    <td>translate text from one language into another</td>\n",
        "    <td>pipeline(task=“translation”)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Text classification</td>\n",
        "    <td>assign a label to a given sequence of text</td>\n",
        "    <td>pipeline(task=“sentiment-analysis”)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Text generation</td>\n",
        "    <td>generate text that follows a given prompt</td>\n",
        "    <td>pipeline(task=“text-generation”)</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Image Classification</td>\n",
        "    <td>assign a label to an image</td>\n",
        "    <td>pipeline(task=“image-classification”)</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "For a comprehensive overview you can click [here](https://huggingface.co/docs/transformers/main/en/quicktour#pipeline).\n"
      ],
      "metadata": {
        "id": "YYieC5X47Gfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first example we will explore the pipeline for translating text from one language to another one (use `pipeline(\"translation_xx_to_yy\")`) For example, to translate from English to German you can use `pipeline(\"translation_en_to_de\")`. \n",
        "\n",
        "The pipeline downloads and caches a default pretrained model. You can then use it on your target text, e.g., `response = translator(text)`.  "
      ],
      "metadata": {
        "id": "M7P3zBzF_XjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation_en_to_de\")\n",
        "\n",
        "input_text = \"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. \"\n",
        "translated_text = translator(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "id": "fYK429avs9qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than using a default model, we will now use a **specific model** for the translation task and pass the model, which should be used for the translation to the pipeline as parameter. \n",
        "\n",
        "We will use `mdl_name = \"Helsinki-NLP/opus-mt-en-de\"`. You can find details concerning the model on its [model card](https://huggingface.co/Helsinki-NLP/opus-mt-en-de) in the ModelHub."
      ],
      "metadata": {
        "id": "a3JR2dZjBIDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdl_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "translator = pipeline(\"translation\", model=mdl_name)\n",
        "\n",
        "input_text = \"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. \"\n",
        "translated_text = translator(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "id": "Ptw8YMDKA6ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gradio library which we already used during the last session allows to load almost every model on HuggingFace. For this, we use the class level method `load` (i.e., `gradio.Interface.load(name, ···)`). The input and output components of the user interface are then automatically loaded from the repository. "
      ],
      "metadata": {
        "id": "h2TK5TinDjAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "\n",
        "grad.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-en-de\").launch()"
      ],
      "metadata": {
        "id": "BvfaAAHkAmMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can also load the user interface from a pipeline. For this, we use the class level method `from_pipeline` (i.e., `gradio.Interface.from_pipeline(name, ···)`). The input and output components of the user interface are then automatically determined from the pipeline. "
      ],
      "metadata": {
        "id": "cLrMk4Rg0Df5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "\n",
        "grad.Interface.from_pipeline(translator).launch()"
      ],
      "metadata": {
        "id": "RZYCPZok0D9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can configure title and description of the user interface and also provide examples"
      ],
      "metadata": {
        "id": "bt-9jtnT0gsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "\n",
        "title = \"Translation\"\n",
        "description = \"Translates text from English to German ... \"\n",
        "examples = [[\"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch.\"]]\n",
        "\n",
        "grad.Interface.from_pipeline(translator, title=title, description=description, examples=examples).launch()"
      ],
      "metadata": {
        "id": "NaUYLdkW1Hg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Transformers for Text Classification "
      ],
      "metadata": {
        "id": "Yhj9DS8NL6X8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a next step we use the Transformers library for a text classification task (using `pipeline(\"sentiment-analysis\")`). As model we use a specific model (i.e., \n",
        "`mdl_name = \"siebert/sentiment-roberta-large-english\"`), which is a fine-tuned checkpoint of a RoBERTa large model. "
      ],
      "metadata": {
        "id": "w0JThuAOE3Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as grad\n",
        "\n",
        "mdl_name = \"siebert/sentiment-roberta-large-english\"\n",
        "roberta_pipe = pipeline(\"sentiment-analysis\", model=mdl_name)\n",
        "\n",
        "title = \"Sentiment Analysis\"\n",
        "description = \"assigns a label to a given sequence of text ... \"\n",
        "examples = [[\"I am super happy today\"], [\"The weather is awful\"],]\n",
        "\n",
        "grad.Interface.from_pipeline(roberta_pipe, title=title, description=description, examples=examples).launch(debug=True)"
      ],
      "metadata": {
        "id": "zGC_QC51_dnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Transformers for Text Generation"
      ],
      "metadata": {
        "id": "HK-WMfaXFH0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a next step we use the Transformers library for a text generation task (using `pipeline(\"text-generation\")`).\n",
        "\n",
        "As model we use a specific model (i.e., `model = \"GPT2\"`). We will configure the `generator` by limiting the length of the output (using `max_length`) and by specifying the number of sequences that should be generated (using `num_return_sequences`)."
      ],
      "metadata": {
        "id": "sG6EZEbwGGpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import gradio as grad\n",
        "\n",
        "def generate(text):\n",
        "  generator = pipeline('text-generation', model='gpt2')\n",
        "  set_seed(42)\n",
        "  generated_text = generator(text, max_length=30, num_return_sequences=1)\n",
        "  return generated_text[0][\"generated_text\"]\n",
        "\n",
        "examples = [[\"Once upon a time\"]]\n",
        "txt=grad.Textbox(lines=1, label=\"Your input\", placeholder=\"Once upon\")\n",
        "grad.Interface(generate, \n",
        "               inputs=txt, \n",
        "               outputs=\"text\",\n",
        "               examples=examples).launch()"
      ],
      "metadata": {
        "id": "zUI-DDq-E9mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Transformers for Text Summarization"
      ],
      "metadata": {
        "id": "Xmcs_22hKpxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now try a text summarization task. As pre-trained model we will rely on `deep-learning-analytics/wikihow-t5-small`. For more info on the model you can visit the [ModelHub](https://huggingface.co/deep-learning-analytics/wikihow-t5-small).\n",
        "\n",
        "**Create your own pipeline**\n",
        "\n",
        "Since the default pipelines only support limited scenarios we now look into how to create our own pipeline.\n",
        "\n",
        "In a text summarization task we have a piece of text as an input and would like to return a summary of this text. However, we cannot feed this input text directly into the language model. The language model expects a tensor with the IDs referring to the token indices.  For this we use a tokenizer that can help with (1) splitting the text into words and sub-words and (2) mapping each token to an integer.\n",
        "\n",
        "On a more technical level we use a `Tokenizer`, more specifically an `AutoTokenizer`, which we can instantiate from an existing file on the Hugging Face Hub (i.e., `tokenizer = AutoTokenizer.from_pretrained(model_name)`). \n",
        "\n",
        "A `Tokenizer` can then be used to encode the input text (here `text_input_sequence`) and provide the encoding required by the language model (e.g., `encoded_text = tokenizer.encode(text_input_sequence, return_tensors=\"pt\")`; `pt` stands for Pytorch). \n"
      ],
      "metadata": {
        "id": "kYzRXhxMK5zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import gradio as grad\n",
        "\n",
        "model_name = \"deep-learning-analytics/wikihow-t5-small\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "text_input_sequence = \"Hello, my name is Barbara\"\n",
        "#Longer text snippet for experimentation - taken from https://huggingface.co/deep-learning-analytics/wikihow-t5-small\n",
        "#text_input_sequence = \"\"\"Lack of fluids can lead to dry mouth, which is a leading cause of bad breath. Water can also dilute any chemicals in your mouth or gut that are causing bad breath., Studies show that eating 6 ounces of yogurt a day reduces the level of odor-causing compounds in the mouth. In particular, look for yogurt containing the active bacteria Streptococcus thermophilus or Lactobacillus bulgaricus., The abrasive nature of fibrous fruits and vegetables helps to clean teeth, while the vitamins, antioxidants, and acids they contain improve dental health.Foods that can be particularly helpful include:Apples — Apples contain vitamin C, which is necessary for health gums, as well as malic acid, which helps to whiten teeth.Carrots — Carrots are rich in vitamin A, which strengthens tooth enamel.Celery — Chewing celery produces a lot of saliva, which helps to neutralize bacteria that cause bad breath.Pineapples — Pineapples contain bromelain, an enzyme that cleans the mouth., These teas have been shown to kill the bacteria that cause bad breath and plaque., An upset stomach can lead to burping, which contributes to bad breath. Don’t eat foods that upset your stomach, or if you do, use antacids. If you are lactose intolerant, try lactase tablets., They can all cause bad breath. If you do eat them, bring sugar-free gum or a toothbrush and toothpaste to freshen your mouth afterwards., Diets low in carbohydrates lead to ketosis — a state in which the body burns primarily fat instead of carbohydrates for energy. This may be good for your waistline, but it also produces chemicals called ketones, which contribute to bad breath.To stop the problem, you must change your diet. Or, you can combat the smell in one of these ways:Drink lots of water to dilute the ketones.Chew sugarless gum or suck on sugarless mints.Chew mint leaves.\"\"\"\n",
        "encoded_text = tokenizer.encode(text_input_sequence,return_tensors=\"pt\") #pt for pytorch and tf for tensorflow\n",
        "print(encoded_text)\n",
        "\n",
        "#For testing purposes we use the method convert_ids_to_tokens to convert the ids into tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded_text[0])\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "4Qg229b5Y1lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have looked into how a `Tokenizer` can be used, we will look into how we can use the language model to generate a summary. \n",
        "\n",
        "First we download a pre-trained model the same as we downloaded the tokenizer in the above step and instantiate the model. \n",
        "\n",
        "```\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "```\n",
        "Then we pass the encoded text as a tensor to the language model and use the model to generate the summary (i.e., `summary_ids`). The following parameters are used: \n",
        "\n",
        "*  `tkn_text` representing the tensor with the IDs\n",
        "*  `max_length` allows to restrict the length of the generated token\n",
        "* `num_beams` number of beams for beam search; default 1 (means no beam search)\n",
        "* `repetition_penalty` is the parameter for repetition penality; default 1.0 stands for no penalty \n",
        "* `length_penalty` is the parameter for the length penalty; values > 0 promote longer sequences; values < 0 shorter sequences\n",
        "* `early_stopping` defaults to False - When `True`, generation finishes if the EOS token is reached\n",
        "\n",
        "```\n",
        "summary_ids = model.generate(\n",
        "          tkn_text,\n",
        "          max_length=250, \n",
        "          num_beams=5,\n",
        "          repetition_penalty=2.5, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        ")\n",
        "```\n",
        "\n",
        "Finally, we use the `Tokenizer` to decode the result provided by the language model (i.e., the tensor with the IDs) into text, i.e., `tokens = tokenizer.decode(encoded_text[0], skip_special_tokens=True)`. "
      ],
      "metadata": {
        "id": "7pY1ZmQbrCDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import gradio as grad\n",
        "\n",
        "# Define the model repo\n",
        "model_name = \"deep-learning-analytics/wikihow-t5-small\" \n",
        "\n",
        "# Download model\n",
        "txt2txt_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def text2text_summary(paragraph):\n",
        "    #Remove line breaks\n",
        "    initial_txt = paragraph.strip().replace(\"\\n\",\"\")\n",
        "    #Encode initial_text as tensor\n",
        "    tkn_text = txt2txt_tokenizer.encode(initial_txt,return_tensors=\"pt\") \n",
        "    #Generate summary using model\n",
        "    summary_ids = model.generate(\n",
        "            tkn_text,\n",
        "            max_length=250, \n",
        "            num_beams=5,\n",
        "            repetition_penalty=2.5, \n",
        "            length_penalty=1.0, \n",
        "            early_stopping=True\n",
        "    )\n",
        "    #print(summary_ids)\n",
        "    #Decode the list of IDs \n",
        "    response = txt2txt_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "#Examples\n",
        "examples = [[\"Lack of fluids can lead to dry mouth, which is a leading cause of bad breath. Water can also dilute any chemicals in your mouth or gut that are causing bad breath., Studies show that eating 6 ounces of yogurt a day reduces the level of odor-causing compounds in the mouth. In particular, look for yogurt containing the active bacteria Streptococcus thermophilus or Lactobacillus bulgaricus., The abrasive nature of fibrous fruits and vegetables helps to clean teeth, while the vitamins, antioxidants, and acids they contain improve dental health.Foods that can be particularly helpful include:Apples — Apples contain vitamin C, which is necessary for health gums, as well as malic acid, which helps to whiten teeth.Carrots — Carrots are rich in vitamin A, which strengthens tooth enamel.Celery — Chewing celery produces a lot of saliva, which helps to neutralize bacteria that cause bad breath.Pineapples — Pineapples contain bromelain, an enzyme that cleans the mouth., These teas have been shown to kill the bacteria that cause bad breath and plaque., An upset stomach can lead to burping, which contributes to bad breath. Don’t eat foods that upset your stomach, or if you do, use antacids. If you are lactose intolerant, try lactase tablets., They can all cause bad breath. If you do eat them, bring sugar-free gum or a toothbrush and toothpaste to freshen your mouth afterwards., Diets low in carbohydrates lead to ketosis — a state in which the body burns primarily fat instead of carbohydrates for energy. This may be good for your waistline, but it also produces chemicals called ketones, which contribute to bad breath.To stop the problem, you must change your diet. Or, you can combat the smell in one of these ways:Drink lots of water to dilute the ketones.Chew sugarless gum or suck on sugarless mints.Chew mint leaves.\"]]\n",
        "\n",
        "\n",
        "#Gradio User Interface\n",
        "para=grad.Textbox(lines=10, label=\"Paragraph\", placeholder=\"Add your text here\")\n",
        "out=grad.Textbox(lines=1,label=\"Summary\")\n",
        "grad.Interface(text2text_summary, \n",
        "               inputs=para, \n",
        "               outputs=out, \n",
        "               examples=examples).launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "PpLogMKsENBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Chatbot/Dialog Bot"
      ],
      "metadata": {
        "id": "FdNPiBY1OyBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now create a chatbot with HuggingFace Transformers. \n",
        "\n",
        "First of all, we load a tokenizer and a model instance for a specific variant of DialoGPT (e.g., `microsoft/DialoGPT-large`, `microsoft/DialoGPT-medium` or `microsoft/DialoGPT-small`). The tokenizer will help with encoding and decoding and the model will be used to generate responses. For our purposes we will use the `AutoTokenizer` and the `AutoModelForCausalLM`. \n",
        "\n",
        "Then we define a function `converse`. Using the tokenizer, the model, and the chat_history, a response to the user input is generated. In a first step, the user input and an End-of-String (EOS) token are encoded. Then in a second step, these are appended to the chat history. The 1000 most recent tokens (i.e.,  `max_length=1000`) are then used in a third step by DialogGPT for generating responses. Finally, the response is then decoded and transformed into tuples (i.e., pairs of user input and corresponding answer by the bot) as needed by the user interface."
      ],
      "metadata": {
        "id": "BoY9GUBao0bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "def converse(user_input, chat_history=[]):\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    user_input_ids = tokenizer.encode(user_input,tokenizer.eos_token,return_tensors='pt')\n",
        " \n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([torch.LongTensor(chat_history), user_input_ids], dim=-1) \n",
        "    \n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n",
        "    #print(chat_history)\n",
        "\n",
        "    response = tokenizer.decode(chat_history[0]).split(\"<|endoftext|>\")\n",
        "    #print(response) \n",
        "   \n",
        "    # convert to tuples of list\n",
        "    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  \n",
        "    #print(response) \n",
        "   \n",
        "    return response, chat_history\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "gr.Interface(fn=converse,\n",
        "             theme=\"default\",\n",
        "             inputs=[gr.Textbox(placeholder=\"Let's chat\"), \"state\"],\n",
        "             outputs=[\"chatbot\", \"state\"]).launch(debug=False)\n"
      ],
      "metadata": {
        "id": "WbHhcQiyO1Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have seen how a chatbot can be created using the Transformers libary, we will use Gradio to automatically create a user interface for two different models (`microsoft/DialoGPT-large` and `facebook/blenderbot-400M-distill`). This requires very little programming skills. At the same time there is less possibility to configure the chatbot.\n"
      ],
      "metadata": {
        "id": "y3gW35jFElEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "grad.Interface.load(\"huggingface/microsoft/DialoGPT-large\").launch()"
      ],
      "metadata": {
        "id": "hNDA9uHYDmu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "grad.Interface.load(\"huggingface/facebook/blenderbot-400M-distill\").launch()"
      ],
      "metadata": {
        "id": "k79Jm4-YEOWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Multiple Models and Combining Multiple ML Tasks\n"
      ],
      "metadata": {
        "id": "JoqOs_i7MjZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparing Models using Gradio\n",
        "\n",
        "Gradio can also be used to load several models in parallel to compare them."
      ],
      "metadata": {
        "id": "REQvr0IgKgmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "from gradio.mix import Parallel, Series\n",
        "\n",
        "examples = [[\"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch.\"]]\n",
        "\n",
        "io1 = grad.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-en-de\")\n",
        "io2 = grad.Interface.load(\"huggingface/t5-base\")\n",
        "Parallel (io1,io2, examples=examples).launch(debug=True)"
      ],
      "metadata": {
        "id": "f1KlW93_I6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Multiple Models using Gradio\n",
        "\n",
        "We can also use Gradio to combine multiple models to more complex applications. For example, we can with just a few lines create a simple application to translate German text to English and to summarize it."
      ],
      "metadata": {
        "id": "f7lmwUKEM361"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "\n",
        "examples = [[\"\"\"Haben Hausarbeiten eine Zukunft?\n",
        "Dank ChatGPT ist jeder Text im Nu geschrieben. Es ist an der Zeit, über Rolle und Bedeutung von Hausarbeiten an Schulen und Universitäten nachzudenken. Ein Beitrag von Vito Roberto und Roman Schister.\n",
        "«Schriftliche Arbeiten sind ein wichtiger Bestandteil der Ausbildung, da sie den Schülern und Studenten helfen, ihr Verständnis bestimmter Themen zu vertiefen und ihre Fähigkeiten im Schreiben und Argumentieren zu verbessern. Sodann ermöglichen sie den Lehrern, die Leistungen der Schüler und Studenten zu bewerten und ihnen Feedback zu geben, damit sie ihre Kenntnisse und Fähigkeiten verbessern können.»\n",
        "\n",
        "Den vorstehenden Absatz hat der Chatbot «ChatGPT» geschrieben, den das US-amerikanische Unternehmen OpenAI letzten November veröffentlicht hat. Das Programm musste lediglich gefragt werden: «Welche Bedeutung haben schriftliche Arbeiten in der Ausbildung?» Das Ergebnis lässt sich sehen: Der produzierte Text enthält für die gestellte Frage wesentliche Punkte, ist kohärent, sprachlich korrekt und könnte auch von einem Menschen verfasst worden sein.\"\"\"]]\n",
        "\n",
        "translator = grad.Interface.load(\"huggingface/Helsinki-NLP/opus-mt-de-en\")\n",
        "summarizer = grad.Interface.load(\"huggingface/deep-learning-analytics/wikihow-t5-small\")\n",
        "Series(translator, summarizer, inputs=grad.Textbox(lines=10, label=\"Input Text in German\", placeholder=\"Add your text here\"), examples=examples).launch(debug=True)"
      ],
      "metadata": {
        "id": "t9SCH_VsNNYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Label Detection with Translation (for Google Colab User)\n"
      ],
      "metadata": {
        "id": "p8YKzKLR5-Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now combine the code for label detection via an existing API you already know from the previous session with a translation task."
      ],
      "metadata": {
        "id": "1cWG00ij7LiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Google Cloud Vision library that allows us to make request to the Google Cloud Vision API\n",
        "!pip install google-cloud-vision"
      ],
      "metadata": {
        "id": "QGLOAmJT44OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "from google.cloud import vision\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "V-s5W4bX48DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credentials = {\n",
        "##COPY the content of the JSON file here##   \n",
        "\n",
        "}\n",
        "\n",
        "    \n",
        "json_credentials = json.dumps(credentials) \n",
        "\n",
        "with open('My Project-543e6ed386ee.json','w') as outfile:\n",
        "  outfile.write(json_credentials)"
      ],
      "metadata": {
        "id": "x2kKXNGVIxMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the GOOGLE_APPLICATION_CREDENTIALS environment variable the location of a credential JSON file can be provided. \n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'My Project-543e6ed386ee.json'"
      ],
      "metadata": {
        "id": "3qDuDNKH487B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the client (this only works with the credantials correctly set)\n",
        "client = vision.ImageAnnotatorClient()"
      ],
      "metadata": {
        "id": "7hfbrJOo5BR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "from PIL import Image as PillowImage\n",
        "import io"
      ],
      "metadata": {
        "id": "C6c2R_wv-xRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapted from https://stackoverflow.com/questions/33101935/convert-pil-image-to-byte-array\n",
        "# Conversion of image into byte array\n",
        "def image_to_byte_array(filePath) -> bytes:\n",
        "  #Open File as PIL Image\n",
        "  pil = PillowImage.open(filePath, \"r\")   \n",
        "  # BytesIO is a fake file stored in memory\n",
        "  imgByteArr = io.BytesIO()\n",
        "  # image.save expects a file as a argument, passing BytesIO object\n",
        "  pil.save(imgByteArr, format=pil.format)\n",
        "  # Turn the BytesIO object back into a bytes object\n",
        "  imgByteArr = imgByteArr.getvalue()\n",
        "  return imgByteArr"
      ],
      "metadata": {
        "id": "fqrg1jnI-tg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def detect_and_translate_text(filePath, translator):\n",
        "  image = vision.Image()\n",
        "  image.content = image_to_byte_array(filePath)\n",
        "  response_text = client.text_detection(image=image)\n",
        "\n",
        "  text=\"\"\n",
        "  if response_text.text_annotations:\n",
        "    text = response_text.text_annotations[0].description\n",
        "  \n",
        "  #Translate output of label detection\n",
        "  mdl_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "  translator = pipeline(\"translation\", model=mdl_name)\n",
        "  response = translator(text)\n",
        "  \n",
        "  return response"
      ],
      "metadata": {
        "id": "-WlwQ2T-5EEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as grad\n",
        "demo = grad.Interface(\n",
        "    detect_and_translate_text, \n",
        "    inputs=grad.Image(type=\"filepath\"),\n",
        "    outputs = grad.TextArea()\n",
        ")\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "l9lDfvlk-jPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "qWtiHCXyD8LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try for yourself and get your hands dirty.\n",
        "\n",
        "Modify the example to translate from english to spanish. Search the [ModelHub](https://huggingface.co/models) to find a model which allows translating from english to spanish."
      ],
      "metadata": {
        "id": "aQ1r4o3CD7vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Add your code to translate from English to Spanish here including the user interface\n",
        "\n",
        "from transformers import pipeline\n",
        "import gradio as grad\n",
        "mdl_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "opus_translator = pipeline(\"translation\", model=mdl_name)\n",
        "grad.Interface.from_pipeline(opus_translator).launch()\n"
      ],
      "metadata": {
        "id": "foDZ-RVz82QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try to create a user interface for image classifcation. Use the pipeline abstraction with task `image-classification`. As model use `google/mobilenet_v2_1.0_224`.\n"
      ],
      "metadata": {
        "id": "0u8SwvHjF51i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Add your code for image classification here \n",
        "\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"image-classification\", model=\"google/mobilenet_v2_1.0_224\")\n",
        "grad.Interface.from_pipeline(classifier).launch()"
      ],
      "metadata": {
        "id": "TEp0GtfK-cYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}