{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b0ogw5qLxXs"
      },
      "source": [
        "# Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzHVRPyEbKBq"
      },
      "source": [
        "Within this Jupyter notebook we will use **AI as a service** and try an existing service for vision detection. As an example we will use Google's Cloud Vision API.\n",
        "\n",
        "Google Cloud Vision provides **capabilities for vision detection** through **pre-trained ML models**, including image labeling, face and landmark detection, text detection, and tagging of explicit content. To enable easy integration of these capabilities in your application Google Cloud Vision provides a **REST API** and **client libraries** in different programming languages such as Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLcBQl4vKIHX"
      },
      "outputs": [],
      "source": [
        "# Install the Google Cloud Vision library that allows us to make request to the Google Cloud Vision API\n",
        "!pip install google-cloud-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1f0MIkEPTOk"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "from google.cloud import vision\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4GQIYdRdoGY"
      },
      "source": [
        "To use the Google Vision API you require **access keys**. The access keys contain authorization information for using the API, but no identity information. Each time you want to make a call to Google's Cloud Vision API an access key needs to be provided (in form of a JSON file) which is then used to authenticate and provide authorization information to Google.\n",
        "\n",
        "**Get your private key file.** For the sake of this tutorial session you can download the JSON file with the private key information from Canvas. If you want to continue using this notebook after the course, you can set up the Vision API for yourself and create your own service account keys (https://cloud.google.com/vision/docs/setup).\n",
        "\n",
        "**Copy the content of the private key file into the cell below.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXHxlz0hZxih"
      },
      "outputs": [],
      "source": [
        "credentials = {\n",
        "##COPY the content of the JSON file here##\n",
        "\n",
        "}\n",
        "\n",
        "json_credentials = json.dumps(credentials)\n",
        "\n",
        "with open('My Project-543e6ed386ee.json','w') as outfile:\n",
        "  outfile.write(json_credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oai28UCLyPS"
      },
      "outputs": [],
      "source": [
        "# Using the GOOGLE_APPLICATION_CREDENTIALS environment variable the location of a credential JSON file can be provided.\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'My Project-543e6ed386ee.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLO-Mc3ZWq3l"
      },
      "source": [
        "# Using Google's Vision API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf49xZD4psav"
      },
      "source": [
        "Google's [Cloud Vision API](https://cloud.google.com/vision/docs) provides different vision detection capabilities via a **single endpoint** that supports the annotation of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee9Bx329L3_H"
      },
      "outputs": [],
      "source": [
        "# Instantiate the client (this only works with the credantials correctly set)\n",
        "client = vision.ImageAnnotatorClient()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufYjYY8Rr5Ke"
      },
      "source": [
        "Each request to the API requires an **image**. One way to provide the image is by specifying the image URI. This can either be a publicly-accessible HTTP or HTTPS URL or a Cloud Storage URI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q877CFY-M84K"
      },
      "outputs": [],
      "source": [
        "# Here we use a publicly-accessible URL as image URI\n",
        "# Before making the request we open the image via its uri and display it\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "uri = 'https://www.indiewire.com/wp-content/uploads/2016/08/20140216-131646.jpg'\n",
        "#uri = 'https://cdn.qwiklabs.com/5%2FxwpTRxehGuIRhCz3exglbWOzueKIPikyYj0Rx82L0%3D'\n",
        "\n",
        "with urllib.request.urlopen(uri) as url:\n",
        "    img=Image.open(url)\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G9P1lqKuXEx"
      },
      "source": [
        "Initialize the image and pass the URI as the value of `image.source.image_uri`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xMx-fUEuaTy"
      },
      "outputs": [],
      "source": [
        "# Set image to be analyzed by Google Vision\n",
        "image = vision.Image()\n",
        "image.source.image_uri=uri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj5YMcLa-jWu"
      },
      "source": [
        "## Face Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-Ju6egunsG"
      },
      "source": [
        "A request to the API additionally requires an indication concerning the **types of annotations** to be performed on the image.\n",
        "\n",
        "For example, `client.face_detection(image=image)` can be used to use the capability of **detecting faces** in images. For a more comprehensive overview of supported feature types see https://cloud.google.com/vision/docs/features-list.\n",
        "\n",
        "The **face_detection** feature\n",
        "*   locates faces with bounding polygons,\n",
        "*   identifies specific facial \"landmarks\" such as eyes, ears, nose, mouth\n",
        "\n",
        "along with their corresponding confidence values.\n",
        "\n",
        "Moreover it returns\n",
        "* likelihood ratings for emotion (joy, sorrow, anger, surprise) and\n",
        "* general image properties (underexposed, blurred, headwear present).\n",
        "\n",
        "Likelihoods ratings are expressed as 6 different values: UNKNOWN, VERY_UNLIKELY, UNLIKELY, POSSIBLE, LIKELY, or VERY_LIKELY.\n",
        "\n",
        "The API provides this information in the response object which is represented in JSON format. The response can be stored in a variable, e.g., `response_faces` to access the above information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYyrmtsHwaKn"
      },
      "outputs": [],
      "source": [
        "#### FACE DETECTION ######\n",
        "## Use the face detection feature of the Google Cloud API and store the result\n",
        "response_faces = client.face_detection(image=image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TwKSDAc_BWx"
      },
      "source": [
        "Let us now inspect the JSON response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "575mkhtE-_Uw"
      },
      "outputs": [],
      "source": [
        "##Show the JSON response\n",
        "print(response_faces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQyw6jRyU4N"
      },
      "source": [
        "Since the JSON response provides more information than what we actually need we can further process the response and only extract the data we are interested in.\n",
        "\n",
        "**How to access a face?**\n",
        "The JSON response can potentially contain more than one face. With `response_faces.face_annotations` we can access the different faces and use a loop `for face in response_faces.face_annotations:` to iterate over the different faces. With `response_faces.face_annotations[0]` we can access the first face.\n",
        "\n",
        "**How to access the properties of a single face?**\n",
        "This then allows us to access the properties of each of the faces. For example, `face.surprise_likelihood ` gives us the surprise likelihood of a particular face.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoVzpkkNBhdx"
      },
      "source": [
        "### Extracting Face Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF3r8ece8UZh"
      },
      "outputs": [],
      "source": [
        "for face in response_faces.face_annotations:\n",
        "    print(face.surprise_likelihood)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmauNHP_AHIL"
      },
      "source": [
        "To make the output look a bit more user-friendly and only show the likelihood name we make a small change to the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTNZ8lm7AGDV"
      },
      "outputs": [],
      "source": [
        "# Names of likelihood from google.cloud.vision.enums\n",
        "likelihood_name = ('UNKNOWN', 'VERY_UNLIKELY', 'UNLIKELY', 'POSSIBLE',\n",
        "                       'LIKELY', 'VERY_LIKELY')\n",
        "\n",
        "for face in response_faces.face_annotations:\n",
        "    print(likelihood_name[face.surprise_likelihood])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcCZDbHF9Im0"
      },
      "source": [
        "\n",
        "Let us now create a string with information concerning all emotion likelihood ratings. For this, wedefine a function called `hasEmotions(face)` which returns for a given face a string with the likelihood ratings for the different emotions (e.g., `Joy: VERY_LIKELY; Sorrow: VERY_UNLIKELY; Anger: VERY_UNLIKELY; Surprise: VERY_UNLIKELY`).\n",
        "\n",
        "Moreover, let us define a function called `obtainCharacteristics(face)` which returns for a given face a string with the headware likelihood rating (e.g., `Wearing headware: VERY_LIKELY`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKVBdzvoPe7w"
      },
      "outputs": [],
      "source": [
        "# Function to create a string with the likelihood rating for the different emotions\n",
        "def hasEmotions(face):\n",
        "      emotionStr = 'Joy: {}; Sorrow: {}; Anger: {}; Surprise: {}'.format(likelihood_name[face.joy_likelihood],\n",
        "                                                                         likelihood_name[face.sorrow_likelihood],\n",
        "                                                                         likelihood_name[face.anger_likelihood],\n",
        "                                                                         likelihood_name[face.surprise_likelihood])\n",
        "      return emotionStr\n",
        "\n",
        "# Function to create a string with the headware likelihood rating\n",
        "def obtainCharacteristics(face):\n",
        "        fmtstr = 'Wearing headware: {}'.format(likelihood_name[face.headwear_likelihood])\n",
        "        return fmtstr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4JbtG8w4kA3"
      },
      "source": [
        "Since the response provided by the API can potentially contain several faces the function defined above needs to be call for each face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwfkuswrPGeI"
      },
      "outputs": [],
      "source": [
        "# For each face the emotion and headware wearing likelihoods are shown\n",
        "for face in response_faces.face_annotations:\n",
        "   print(hasEmotions(face))\n",
        "   print(obtainCharacteristics(face))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32sO0tH-zO9b"
      },
      "outputs": [],
      "source": [
        "# Show the emotion and headware wearing likelihoods for the first face\n",
        "face = response_faces.face_annotations[0]\n",
        "print(hasEmotions(face))\n",
        "print(obtainCharacteristics(face))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8rP_SalPFsq"
      },
      "source": [
        "## Label Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V373n2qMP62T"
      },
      "source": [
        "We already tried the face detection features. Let's now explore some of the other **types of annotations**.\n",
        "\n",
        "For example, `client.label_detection(image=image)` can be used to use the capability of **detecting labels** in images. For a more comprehensive overview of supported feature types see https://cloud.google.com/vision/docs/features-list.\n",
        "\n",
        "The `label_detection` feature provides generalized labels for an image.\n",
        "For each label it returns\n",
        "* a textual description,\n",
        "* confidence score,\n",
        "* and topicality rating.\n",
        "\n",
        "If you want to use customized labeling you can use [AutoML Vision](https://cloud.google.com/vision/automl/docs) to create a custom machine learning model for your specific use case. You provide a labeled data set and AutoML Vision does the training for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHYVkzNyQVzk"
      },
      "outputs": [],
      "source": [
        "#### LABEL DETECTION ######\n",
        "\n",
        "response_label = client.label_detection(image=image)\n",
        "#print(response_label)\n",
        "\n",
        "for label in response_label.label_annotations:\n",
        "    print({'label': label.description, 'score': label.score})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bTsOHGSPFwF"
      },
      "source": [
        "## Landmark Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jam9QhbJPAsF"
      },
      "source": [
        "Let's now explore the landmark detection. For this, we need to provide an image that contains a landmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18aKYregb5WK"
      },
      "outputs": [],
      "source": [
        "# Open an image via its uri and display it\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "uri = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQa-ItjJOrVECuwX-xAtN9rNTuNiHl2aHCeSA&usqp=CAU'\n",
        "#uri = 'https://img.luzernerzeitung.ch/2020/5/26/fbc87bef-bf45-433b-a7ca-5404a9ef37ea.jpeg?width=560&fit=crop&quality=75&auto=webp'\n",
        "\n",
        "with urllib.request.urlopen(uri) as url:\n",
        "    img=Image.open(url)\n",
        "    display(img)\n",
        "\n",
        "# Set image to be analyzed by Google Vision\n",
        "image = vision.Image()\n",
        "image.source.image_uri=uri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr8JA11rRTLa"
      },
      "source": [
        "Using `client.landmark_detection(image=image)` can be used to use the capability of detecting landmarks in images.\n",
        "\n",
        "The `landmark_detection` feature provides\n",
        "* the name of the landmark, a confidence score and a bounding box in the image for the landmark,\n",
        "* coordinates for the detected entity.\n",
        "\n",
        "The JSON response contains landmark_annotations. We can access the details for each of the landmark using a for loop that iterates over the annotations: `for landmark in response_image.landmark_annotations:`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOnPw0ytQ0co"
      },
      "outputs": [],
      "source": [
        "#### LANDMARK DETECTION ######\n",
        "\n",
        "response_image = client.landmark_detection(image=image)\n",
        "#print(response_image)\n",
        "\n",
        "for landmark in response_image.landmark_annotations:\n",
        "    print({'label': landmark.description, 'score': landmark.score})\n",
        "    for location in landmark.locations:\n",
        "            lat_lng = location.lat_lng\n",
        "            print('Latitude {}'.format(lat_lng.latitude))\n",
        "            print('Longitude {}'.format(lat_lng.longitude))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17RLJe14Tbh5"
      },
      "source": [
        "## Text Detection in Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOLRKXaJT2rf"
      },
      "source": [
        "Let's now explore optical character recognition (OCR). In particular, `client.text_detection(image=image)` can be used to extract text from images.\n",
        "\n",
        "The `text_detection` feature provides the entire extracted string, as well as individual words, and their bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI2YWpTOdVyR"
      },
      "outputs": [],
      "source": [
        "# Open an image via its uri and display it\n",
        "from PIL import Image as PillowImage\n",
        "import urllib.request\n",
        "\n",
        "uri = 'https://www.inside-digital.de/img/whatsapp-geburtstagssprueche2.jpg?class=1200x900'\n",
        "#uri = 'https://www.galaxus.ch/im/Files/2/8/7/1/1/2/6/5/959002-H-002.xxl3.jpgexportGa4PCo68TlLe9g?impolicy=ProductTileImage&resizeWidth=648&resizeHeight=486&cropWidth=648&cropHeight=486&resizeType=downsize&quality=high'\n",
        "\n",
        "with urllib.request.urlopen(uri) as url:\n",
        "    img=Image.open(url)\n",
        "    display(img)\n",
        "\n",
        "# Set image to be analyzed by Google Vision\n",
        "image = vision.Image()\n",
        "image.source.image_uri=uri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UCA5c00RDcN"
      },
      "outputs": [],
      "source": [
        "#### TEXT DETECTION ######\n",
        "\n",
        "response_text = client.text_detection(image=image)\n",
        "\n",
        "text=\"\"\n",
        "if response_text.text_annotations:\n",
        "  #Only print the entire text, not the individual fragments\n",
        "  text = response_text.text_annotations[0].description\n",
        "print(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}