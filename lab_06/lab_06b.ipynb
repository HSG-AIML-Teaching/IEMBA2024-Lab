{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajNMrbY1MUml"
   },
   "source": [
    "# Using ChatGPT with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUayPEw170rz"
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50s3eLZ7xUnl"
   },
   "source": [
    "In this notebook you will learn how to use ChatGPT with Python for various tasks such as **text generation**, **speech-to-text**,  **text-to-speech**, and  **image generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48SSfF_F8ADV"
   },
   "source": [
    "## Text Generation\n",
    "\n",
    "OpenAI's text generation **models** (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The latest models, `gpt-4` and `gpt-3.5-turbo`, are accessed through the **chat completions API endpoint**.\n",
    "\n",
    "> For an overview of the different models for text generation you can visit https://platform.openai.com/docs/guides/text-generation.\n",
    "\n",
    "The models provide text outputs in response to  inputs given to them. The inputs to these models are also referred to as **\"prompts\"**. Designing a prompt is essentially how you “program” a large language model, usually by providing instructions or some examples of how to successfully complete a task.\n",
    "\n",
    "To use one of these models via the OpenAI API, you can send a request containing the inputs and your API key, and receive a response containing the model's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SartI438kJ7Y"
   },
   "source": [
    "\n",
    "### Chat Completions API\n",
    "\n",
    "Chat models take a list of messages as input and return a model-generated message as output.\n",
    "\n",
    "Before being able to use the Chat Completions API\n",
    "endpoint we need to authenticate ourselves using the **API key**. You can find an API key which will work during this session in Canvas (in the course syllabus).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCNMr6_cm4YC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'COPY THE API_KEY HERE'\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5cipNaSngJa"
   },
   "source": [
    "After successful authentication the Chat Completions API can be used. The API call has two required inputs:\n",
    "\n",
    "* `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`)\n",
    "* `messages`: a list of message objects, where each object has two required fields:\n",
    "   * `role`: the role of the messenger (either system, user, or assistant)\n",
    "   * `content`: the content of the message (e.g., Write me a beautiful poem)\n",
    "\n",
    "There are additional optional parameters:\n",
    "\n",
    "* `temperature`: the temperature parameter allows you to balance randomness and determinism in the output. Low temperature values make the output more focussed and deterministic, while a high temperature increases randomness in the output and produce more unexpected outputs.\n",
    "\n",
    "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
    "\n",
    "While the **system message** helps set the behavior of the assistant, the **user messages** provide requests or comments for the assistant to respond to, and  **assistant messages** store previous assistant responses, but can also be written by you to give examples of desired behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09QaI2PqzQbs"
   },
   "source": [
    "An example Chat Completions API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDQG-s4EzZmM"
   },
   "outputs": [],
   "source": [
    "#Example with a system message\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain generative AI to a 6 year old.\"},\n",
    "\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhhiirKMzcyP"
   },
   "source": [
    "The response format of a Chat Completion API response:\n",
    "\n",
    "* `id:` the ID of the request\n",
    "* `object: `the type of object returned (e.g., chat.completion)\n",
    "* `created:` the timestamp of the request\n",
    "* `model:` the full name of the model used to generate the response\n",
    "* `usage:` the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "* `choices: `a list of completion objects (only one, unless you set n greater than 1)\n",
    "* `message:` the message object generated by the model, with role and content\n",
    "* `finish_reason:` the reason the model stopped generating text (either stop, or length if max_tokens limit was reached)\n",
    "* `index: `the index of the completion in the list of choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slptaVyu1lDu"
   },
   "source": [
    "We can extract the reply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmcoFXzmzcdg"
   },
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXZ9yATI1wmz"
   },
   "source": [
    "### Some Prompting Tipps\n",
    "\n",
    "The next section provides some prompting tipps. For more details you can visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9p6YUWm0v_Z"
   },
   "source": [
    "#### System Messages\n",
    "\n",
    "The system message can be used to prime the assistant with different personalities or behaviors.\n",
    "\n",
    "System messages can be used for\n",
    "*   defining roles\n",
    "*   setting the tone\n",
    "*   specifying the format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5XB-hzr9o5H"
   },
   "source": [
    "NOTE: Be aware that some models do not generally pay as much attention to the system message equally. For example, gpt-3.5-turbo-0301 does not generally pay as much attention to the system message as gpt-4-0314 or gpt-3.5-turbo-0613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkXo-xroy0NF"
   },
   "outputs": [],
   "source": [
    "#System message defining the role\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a computer science teacher.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain generative AI to a 6 year old.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_mq42I14L3E"
   },
   "outputs": [],
   "source": [
    "#System message setting the tone\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a humorous AI. Your task is to write a fun, engaging article about hiking.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7w4twSTyzO8_"
   },
   "outputs": [],
   "source": [
    "#System message specifying the format\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a computer science teacher. Provide 1) a list of advantages of using generative AI followed by 2) a list of disadvantages and risk.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rc5EogxoZnnb"
   },
   "outputs": [],
   "source": [
    "#System message specifying the format\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a teacher teaching executive MBA students. 1) Explain what ML APIs such as Google Cloud Vision or Chat GPT are 2) Explain what open-source, off-the shelf pre-trained models are. 3) Compare the usage of ML APIs versus open-source, off-the shelf pretrained models.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nWaZASY05os"
   },
   "source": [
    "#### Few-shot Prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SGDF_Uaz4k2"
   },
   "outputs": [],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "# Example adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPgGLoLN2Qgj"
   },
   "source": [
    "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can try setting the `name` field of `system` messages to `example_user` and `example_assistant`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUXAW3Dm2P0-"
   },
   "outputs": [],
   "source": [
    "# The business jargon translation example, but with example names for the example messages\n",
    "# Example adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COycJ3lhbqt3"
   },
   "outputs": [],
   "source": [
    "# description: classify titles into categories\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that classifies titles into categories.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_user\", \"content\": \"Paris auf der Saslong nicht zu biegen\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Ski Alpin\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Kobayashi geht in Führung\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Skispringen\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_user\", \"content\": \"Hütter fährt in Abfahrt aufs Podest\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Ski Alpin\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_user\", \"content\": \"Schweizer Freudentag: Flury brilliert vor Hählen\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Ski Alpin\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_user\", \"content\": \",Arsenal unterliegt Tottenham im Derby\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_assistant\", \"content\": \"Fussball\"},\n",
    "        {\"role\": \"user\", \"content\": \"Seoanes Gladbach muss sich mit Remis begnügen\"},\n",
    "        #{\"role\": \"user\", \"content\": \"Superadler Kraft: Endlich einmal gelbe Weihnachten\"},\n",
    "\n",
    "    ],\n",
    "\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPh9-whtnxMM"
   },
   "source": [
    "### Using Chat Completions for Specific Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRIFkT3Uww7Y"
   },
   "source": [
    "#### Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlSiqZpvn0A4"
   },
   "outputs": [],
   "source": [
    "def translateFromEnglishToGerman(text):\n",
    "  response = client.chat.completions.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": text\n",
    "    }\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4-PKRbBoNgJ"
   },
   "outputs": [],
   "source": [
    "response = translateFromEnglishToGerman(\"My name is Barbara. What is yours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSXYuZZUo_SC"
   },
   "outputs": [],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9qDuCeFnGsz"
   },
   "source": [
    "#### Using ChatGPT for Image Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AX8KmJOmCq5"
   },
   "outputs": [],
   "source": [
    "def createImagePrompt(text):\n",
    "  response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "  ]\n",
    "  )\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3XfFfSCrNwA"
   },
   "outputs": [],
   "source": [
    "response = createImagePrompt(\"create 1 text to image prompt to create nice and cinematic mountain ranges\")\n",
    "generatedPrompt = response.choices[0].message.content\n",
    "print(generatedPrompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWOx8VRM3BBn"
   },
   "source": [
    "## Text to Speech Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3qwy2nIERKC"
   },
   "source": [
    "The Audio API provides a `speech` endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices (`alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`) and can be used to:\n",
    "\n",
    "* Narrate a written blog post\n",
    "* Produce spoken audio in multiple languages\n",
    "* Give real time audio output using streaming\n",
    "\n",
    "To check out supported languages visit https://platform.openai.com/docs/guides/text-to-speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KGHU8qvFSgM"
   },
   "source": [
    "### Audio Generation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZVxPHejrZXX"
   },
   "outputs": [],
   "source": [
    "def generateTextToSpeech(text):\n",
    "  speech_file_path = \"speech.mp3\"\n",
    "  response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=text\n",
    "  )\n",
    "\n",
    "  response.stream_to_file(speech_file_path)\n",
    "  return speech_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EILjTImd3VQa"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "fileName=generateTextToSpeech(\"Today is a wonderful day to build something people love!\")\n",
    "ipd.Audio(filename=fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymJNZuYwsK6Y"
   },
   "source": [
    "In the next code snippet there is a translation step into the target language (e.g., German) before the audio output is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_0Z7SvqoZ7W"
   },
   "outputs": [],
   "source": [
    "# Here we create a whole simple pipeline starting from translation and then speech generation\n",
    "\n",
    "response = translateFromEnglishToGerman(\"My name is Barbara. What is yours?\")\n",
    "translatedText = response.choices[0].message.content\n",
    "\n",
    "fileName=generateTextToSpeech(translatedText)\n",
    "ipd.Audio(filename=fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TBpd5iXBtWL"
   },
   "source": [
    "## Speech to Text (Audio API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCZ67sevB0lD"
   },
   "source": [
    "The Audio API provides two speech to text endpoints, `transcriptions` and `translations`, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:\n",
    "\n",
    "* Transcribe audio into whatever language the audio is in.\n",
    "* Translate and transcribe the audio into english.\n",
    "\n",
    "File uploads are currently limited to 25 MB and the\n",
    "following input file types are supported: `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, and `webm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7DlsqqTCdDt"
   },
   "source": [
    "### Transcription Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oml-3lwNt9-3"
   },
   "outputs": [],
   "source": [
    "def transcribeAudio(fileName):\n",
    "  audio_file = open(fileName, \"rb\")\n",
    "  transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    "  )\n",
    "  return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will now be asked to upload an .mp3 file which will then be transcribed.\n",
    "\n",
    "To explore the transcription capabilities of the Audio API you can experiment with one of the news audio files available at https://www.srf.ch/audio/nachrichten."
   ],
   "metadata": {
    "id": "6FPkI4thknpg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -O Nachrichten_21-01-2024-1600.1705850582243.mp3 \"https://podcasts.srf.ch/world/audio/Nachrichten_21-01-2024-1600.1705850582243.mp3\""
   ],
   "metadata": {
    "id": "j211Q2G3vOvW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTBllScAvvHZ"
   },
   "outputs": [],
   "source": [
    "# The file name which is passed as a parameter to the function transcribeAudio needs to correspond to the file you have uploaded\n",
    "print(transcribeAudio(\"Nachrichten_21-01-2024-1600.1705850582243.mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQxMTgU0C9k8"
   },
   "source": [
    "Should German not be your mother tongue, then you can use the `translations` endpoint to transcribe the file into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U66NNQ1vDLit"
   },
   "outputs": [],
   "source": [
    "def transcribeAndTranslateAudio(fileName):\n",
    "  audio_file = open(fileName, \"rb\")\n",
    "  transcript = client.audio.translations.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_file,\n",
    "    response_format=\"text\"\n",
    "  )\n",
    "  return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjFKaXsZC8xt"
   },
   "outputs": [],
   "source": [
    "print(transcribeAndTranslateAudio(\"Nachrichten_21-01-2024-1600.1705850582243.mp3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy8C5DdGeqxa"
   },
   "source": [
    "### Working with Audio Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEvphh0dEAlP"
   },
   "source": [
    "By default, the Whisper API only supports files that are less than 25 MB. To work with larger audio files, you will you will need to break them into chunks.\n",
    "\n",
    "One option is to use the **PyDub open source Python package** to split the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0Ef_Lgigs3_"
   },
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkbJu3sVD_Ox"
   },
   "outputs": [],
   "source": [
    "# code adapted from: https://platform.openai.com/docs/guides/speech-to-text/longer-inputs\n",
    "\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def segmentAudio(audioFile):\n",
    "  song = AudioSegment.from_mp3(audioFile)\n",
    "  # PyDub handles time in milliseconds\n",
    "  two_minutes = 2 * 60 * 1000\n",
    "  first_2_minutes = song[:two_minutes]\n",
    "  first_2_minutes.export(\"TwoMinutes\" + audioFile, format=\"mp3\")\n",
    "  return \"TwoMinutes\" + audioFile"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will be asked to upload a file to be transcribed.\n",
    "\n",
    "To explore the segmentation capabilities of the Audio API you can experiment with one longer audio files such as https://www.srf.ch/audio/tagesgespraech."
   ],
   "metadata": {
    "id": "t6-3l6s-nNnh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -O Tagesgespraech_18-01-2024-1300.1705581859927.mp3 \"https://podcasts.srf.ch/world/audio/Tagesgespraech_18-01-2024-1300.1705581859927.mp3\""
   ],
   "metadata": {
    "id": "TLzgTFedv0aj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MinVdrf_jCH8"
   },
   "source": [
    "We can then play the split audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfvSZQwPf1J7"
   },
   "outputs": [],
   "source": [
    "# The file name which is passed as a parameter to the function transcribeAudio needs to correspond to the file you have uploaded\n",
    "fileName = \"Tagesgespraech_18-01-2024-1300.1705581859927.mp3\"\n",
    "twoMinuteFile = segmentAudio(fileName)\n",
    "ipd.Audio(filename=twoMinuteFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-yXHgcDjIgu"
   },
   "source": [
    ".. and even translate it to another language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD9freWohDaY"
   },
   "outputs": [],
   "source": [
    "print(transcribeAndTranslateAudio(twoMinuteFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtjqrJXBjza8"
   },
   "source": [
    "To experiment a bit more with Whisper visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb). Moreover, if you want to learn more on prompting with Whisper visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUISYB-lmySg"
   },
   "source": [
    "## Image Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5jsCk93n8cL"
   },
   "source": [
    "The Images API provides three methods for interacting with images:\n",
    "\n",
    "* Creating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)\n",
    "* Creating variations of an existing image (DALL·E 2 only)\n",
    "* Creating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKCCSklZoFgV"
   },
   "source": [
    "The image generations endpoint allows you to create an original image given a text prompt. When using DALL·E 3, images can have a size of 1024x1024, 1024x1792 or 1792x1024 pixels.\n",
    "\n",
    "By default, images are generated at `standard` quality, but when using DALL·E 3 you can set quality: \"`hd`\" for enhanced detail. Square, standard quality images are the fastest to generate.\n",
    "\n",
    "You can request 1 image at a time with DALL·E 3 (request more by making parallel requests) or up to 10 images at a time using DALL·E 2 with the n parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating images based on a text prompt"
   ],
   "metadata": {
    "id": "cEjy7ZvDbzZN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvUFhEuZm4qS"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=\"a downhill skier in the swiss alps\",\n",
    "  size=\"1024x1024\",\n",
    "  quality=\"standard\",\n",
    "  n=1,\n",
    ")\n",
    "\n",
    "image_url = response.data[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R39ftr4xnXfp"
   },
   "outputs": [],
   "source": [
    "# Here we take the URL created previously and open the image\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "\n",
    "with urllib.request.urlopen(image_url) as url:\n",
    "    img=Image.open(url)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUourTf1p9_z"
   },
   "outputs": [],
   "source": [
    "# set a directory to save DALL·E images to\n",
    "image_dir_name = \"images\"\n",
    "image_dir = os.path.join(os.curdir, image_dir_name)\n",
    "\n",
    "# create the directory if it doesn't yet exist\n",
    "if not os.path.isdir(image_dir):\n",
    "    os.mkdir(image_dir)\n",
    "\n",
    "# print the directory to save to\n",
    "#print(f\"{image_dir=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4Eir2cOp5Lv"
   },
   "outputs": [],
   "source": [
    "# save the image\n",
    "import requests\n",
    "\n",
    "def saveImageAs(generated_image_name):\n",
    "  generated_image_filepath = os.path.join(image_dir, generated_image_name)\n",
    "  generated_image_url = response.data[0].url  # extract image URL from response\n",
    "  generated_image = requests.get(generated_image_url).content  # download the image\n",
    "\n",
    "  with open(generated_image_filepath, \"wb\") as image_file:\n",
    "    image_file.write(generated_image)  # write the image to the file\n",
    "\n",
    "  return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# any name you like; the filetype should be .png\n",
    "generated_image = saveImageAs(\"generated_image.png\")"
   ],
   "metadata": {
    "id": "26MoNswNcSTz"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
